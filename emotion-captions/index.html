 <!DOCTYPE html>
<html lang="en">
<head>
  <title>Contextual Emotion Estimation</title>
  <meta name="description" content="Project page for Contextual Emotion Estimation from Image Captions.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <!-- <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/> -->

  <!--Twitter preview-->
  <!-- <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png"> -->

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Contextual Emotion Estimation from Image Captions</h1>
    <h4>ACII 2023</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>

        <!-- How to write authors and how to do we need links?-->
        <nobr>Vera Yang</nobr><sup></sup> &emsp;
        <nobr>Archita Srivastava</nobr><sup></sup> &emsp;

        
       <nobr>Yasaman Etesam</nobr><sup></sup> </a>&emsp;
       <nobr>Shay Zhang</nobr><sup></sup></a>&emsp;
       <nobr>Angelica Lim</nobr><sup></sup> </a>&emsp;
        <!-- <nobr>Hongwei Zhao</nobr><sup>1</sup> &emsp;
        <nobr>Hongtao Lu</nobr><sup>2</sup> &emsp; -->
        <!-- <a href="https://xishen0220.github.io/"><nobr>Xi Shen</nobr><sup>3</sup></a> -->
      </h4>
      <!-- <sup>*</sup> Equal Contribution &emsp; -->
      <p><em><center>School of Computing Science, Simon Fraser University, Burnaby, BC, Canada  &emsp;</center></em></p>
      <!-- <sup>2</sup> Shanghai Jiao Tong University &emsp;
      <sup>3</sup> Tencent AI Lab, Shenzhen -->
      <!-- Rosie Labs, <nobr>Simon Fraser University</nobr>, <nobr>Burnaby</nobr>, British Columbia,
      <nobr>Canada</nobr> -->
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <!-- <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/monniert/dti-clustering" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div> -->
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.png" alt="teaser.png" class="text-center" style="width: 110%; max-width: 2000px">
  <p><h5><em>Manual Annotation for the given image produces the following caption:</em> Sean is a male adult. Sean is a(n) passenger. Sean is or has raising eyebrows,
    side-eyeing. Mia is a child and she is sitting behind Sean and kicking Sean’s chair. Sean’s physical environment is on an airplane.</h5></p>
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="">Paper</a>
    <a class="label label-info" href="resrc/annotate_no_gpt.csv">Annotations</a>
    <a class="label label-info" href="resrc/annotate_gpt.csv">Annotations  w/Results</a>
    <a class="label label-info" href="https://dev9032.d1ab6be83l9u2m.amplifyapp.com/">Annotation Website</a>
    <a class="label label-info" href="resrc/ref.bib">BibTeX</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    Emotion estimation in images is a challenging task,
    typically using computer vision methods to directly estimate
    people’s emotions using face, body pose and contextual cues. In
    this paper, we explore whether Large Language Models (LLMs)
    can support the contextual emotion estimation task, by first
    captioning images, then using an LLM for inference. First, we
    must understand: how well do LLMs perceive human emotions?
    And which parts of the information enable them to determine
    emotions? One initial challenge is to construct a caption that
    describes a person within a scene with information relevant
    for emotion perception. Towards this goal, we propose a set of
    natural language descriptors for faces, bodies, interactions, and
    environments. We use them to manually generate captions and
    emotion annotations for a subset of 331 images from the EMOTIC
    dataset. These captions offer an interpretable representation for
    emotion estimation, towards understanding how elements of a
    scene affect emotion perception in LLMs and beyond. Secondly,
    we test the capability of a large language model to infer an
    emotion from the resulting image captions. We find that GPT-
    3.5, specifically the text-davinci-003 model, provides surprisingly
    reasonable emotion predictions consistent with human annota-
    tions, but accuracy can depend on the emotion concept. Overall,
    the results suggest promise in the image captioning and LLM
    approach.
  </p>

  <!-- <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-6 text-center">
      <h4><u>Short presentation</u> (3min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/j20MBc1hWGQ" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>

    <div class="col-xs-6 text-center">
      <h4><u>Long presentation</u> (11min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/xhLUOh5PKBA" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

  
  <h3>Approach</h3>
  <hr/>
    
  <div class="row" style="text-align: center">
    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4><u>Image Annotations</u></h4>
        <img src="resrc/annotate.png" alt="pr
        ototypes.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
    </div>

    <div class="row" style="text-align: center">
      <div class="col-xs-6">
        <h4><u>Generated Captions</u></h4>
      </div>
      <div class="col-xs-6">
        <h4><u>Samples</u> 
      </div>
    </div>
    <div class="row" style="text-align: center">
      <div class="col-xs-6">
        <img src="resrc/captions.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 900px">
      </div>
      <div class="col-xs-6">
        <img src="resrc/samples.png" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 900px">
      </div>
    </div>

    <div style="padding-top:3rem">
      <p> Once the annotations were complete, GPT-3 was used
        to predict emotion labels with the help of a prompt. The
        prompt was structured to elicit single emotion prediction when
        presented with an image annotation.
        The prompt was as follows: <em>”Sean is a male adult. Sean is a(n) passenger. Sean
        is or has raising eyebrows, side-eyeing. Mia is a child and
        she is sitting behind Sean and kicking Sean’s chair. Sean’s
        physical environment is on an airplane. Sean is likely feeling
        a high level of {placeholder}? Choose one emotion from
        the list: Anger, Annoyance, Aversion, Confusion, Disapproval,
        Disconnection, Disquietment, Embarrassment, Fatigue, Fear,
        Pain/Suffering (emotional), Pain/Suffering (physical), and Sadness.”</em>
        To evaluate the performance of the models, we compared
        the LLM’s predictions to the ground truth of the images established by the annotators</p>
    </div>
  </div>
  
     

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>GPT-3 Results</u></h4>
      <img src="resrc/gptresults.png" alt="pr
      ototypes.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div  style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4 style="padding-bottom:2rem"><u>Experimental Results</u></h4>
    <div class="row">
      <div class="col-sm-4">
        <img src="resrc/exp1.png" alt="dti.png"  style="width: 110%; max-width: 1100px">
      </div>
      <div class="col-sm-4">
        <img src="resrc/exp2.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1100px">
      </div>
      <div class="col-sm-4">
        <img src="resrc/exp3.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1100px">
      </div>
    </div>
  </div>
  
  <h3>Some Visual Results</h3>
  <hr/>
    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem; padding-top:7rem">
      <img src="resrc/pain2.jpg" alt="deep_tsf.png"  style=" text-align: center;">
      <h4>Ground Truth: <em style ="color:red">Fear</em></h4>
    </div>
    <div class="row" style="text-align: center">
      <h4> GPT-3 Prompting</h4>

      <div class="col-sm-4">
        <h4>Full Caption: <h4 style = "color:#800084">Chloe is a female adult. Chloe is or has frowning, open mouth. Chloe’s physical environment is right in front of an alien hand in the dark. Chloe is likely feeling...? </h4></h4>
        <h4>GPT Prediction: <h4 style="color:red">Fear</em> </h4>
      </div>
  
      <div class="col-sm-4">
        <h4> Minus Interactions:  <h4 style = "color:#800084">Chloe is a female adult. Chloe is or has frowning, open mouth. Chloe's physical environment is right in front of an alien hand in the dark. Chloe is likely feeling...?</h4></h4>
          <h4>GPT Prediction: <em style="color:red">Fear</em> </h4>

      </div>
  
      <div class="col-sm-4">
        <h4> Minus Environment: <h4 style = "color:#800084"> Chloe is a female adult. Chloe is or has frowning, open mouth. Chloe is likely feeling....?</h4>
          <h4>GPT Prediction: <em style="color:red">Disapproval</em> </h4>
        </h4>
      </div>
    
    </div>
    <!-- <h4>Notable Observations: <h5>Fear is an emotion that may benefit from environmental context. The F1 score for this image dropped from 0.52 and 0.48 with
      environments to 0.34 without environments. An example is
      shown in this Figure, when "right in front of an alien hand in
      the dark" was removed from the full caption, the predicted emotion changed
      from Fear to Disapproval. </h5></h4> -->
      
    <hr class="new">


    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem; padding-top:7rem">
      <img src="resrc/cng1.jpg" alt="deep_tsf.png"  style=" text-align: center;">
      <h4>Ground Truth: <em style ="color:green">Embarrassment</em></h4>
    </div>
    <div class="row" style="text-align: center">
      <h4> GPT-3 Prompting</h4>

      <div class="col-sm-4">
        <h4>Full Caption: <h4 style = "color:#800084">Lucas is a male adult. Lucas is a(n) groom. Lucas is or has lips that flatten, palms open. Mia is Lucas’ bride and she is smiling. Lucas’ physical environment is cake falling down 
          at wedding. Lucas is likely feeling....? </h4></h4>
        <h4>GPT Prediction: <em style="color:green">Embarrassment</em> </h4>
      </div>
  
      <div class="col-sm-4">
        <h4> Minus Interactions:  <h4 style = "color:#800084">Lucas is a male adult. Lucas is a(n) groom. Lucas is or has lips that flatten, palms open. Lucas's physical environment is cake falling down at wedding. Lucas is likely feeling....?</h4></h4>
          <h4>GPT Prediction: <em style="color:green">Embarrassment</em> </h4>

      </div>
  
      <div class="col-sm-4">
        <h4> Minus Environment: <h4 style = "color:#800084"> Lucas is a male adult. Lucas is a(n) groom. Lucas is or has lips that flatten, palms open. Mia is Lucas' bride and she is smiling. Lucas is likely feeling....?</h4>
          <h4>GPT Prediction: <em style="color:green">Happiness</em> </h4>
        </h4>
      </div>
    
    </div>
    <!-- <h4>Notable Observations: <h5>Fear is an emotion that may benefit from environmental context. The F1 score for this image dropped from 0.52 and 0.48 with
      environments to 0.34 without environments. An example is
      shown in this Figure, when "right in front of an alien hand in
      the dark" was removed from the full caption, the predicted emotion changed
      from Fear to Disapproval. </h5></h4> -->
      
    <hr class="new">

    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem; padding-top:7rem">
      <img src="resrc/cng2.jpg" alt="deep_tsf.png"  style=" text-align: center;">
      <h4>Ground Truth: <em style ="color:blue">Embarrassment</em></h4>
    </div>
    <div class="row" style="text-align: center">
      <h4> GPT-3 Prompting</h4>

      <div class="col-sm-4">
        <h4>Full Caption: <h4 style = "color:#800084">Jack is a male adult. Jack is or has smiling. Beth is a customer and she is side-eyeing Jack. Zoe is a customer and she is staring at Jack. Jack's physical environment is eating in a movie theatre.
        Jack is likely feeling....? </h4></h4>
        <h4>GPT Prediction: <em style="color:blue">Embarrassment</em> </h4>
      </div>
  
      <div class="col-sm-4">
        <h4> Minus Interactions:  <h4 style = "color:#800084">Jack is a male adult. Jack is or has smiling. Jack's physical environment is eating in a movie theatre. Jack is likely feeling....?</h4></h4>
          <h4>GPT Prediction: <em style="color:blue">Happiness</em> </h4>

      </div>
  
      <div class="col-sm-4">
        <h4> Minus Environment: <h4 style = "color:#800084"> Jack is a male adult. Jack is or has smiling. Beth is a customer and she is side-eyeing Jack. Zoe is a customer and she is staring at Jack. Jack is likely feeling....?</h4>
          <h4>GPT Prediction: <em style="color:blue">Embarrassment</em> </h4>
        </h4>
      </div>
    
    </div>
    <!-- <h4>Notable Observations: <h5>Fear is an emotion that may benefit from environmental context. The F1 score for this image dropped from 0.52 and 0.48 with
      environments to 0.34 without environments. An example is
      shown in this Figure, when "right in front of an alien hand in
      the dark" was removed from the full caption, the predicted emotion changed
      from Fear to Disapproval. </h5></h4> -->
      
    <hr class="new">


    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem; padding-top:7rem">
      <img src="resrc/cng4.jpg" alt="deep_tsf.png"  style=" text-align: center;">
      <h4>Ground Truth: <em style ="color:green">Sadness</em></h4>
    </div>
    <div class="row" style="text-align: center">
      <h4> GPT-3 Prompting</h4>

      <div class="col-sm-4">
        <h4>Full Caption: <h4 style = "color:#800084">Jane is a female adult. Jane is or has taking off eyeglasses. Mia is Jane's daughter and Jane is putting her hand on Mia's shoulder while Mia has her back turned to Jane. Jane's physical environment is on a couch.
        Jane is likely feeling....? </h4></h4>
        <h4>GPT Prediction: <em style="color:green">Sadness</em> </h4>
      </div>
  
      <div class="col-sm-4">
        <h4> Minus Interactions:  <h4 style = "color:#800084">Jane is a female adult. Jane is or has taking off eyeglasses. Jane's physical environment is on a couch. Jane is likely feeling....?</h4></h4>
          <h4>GPT Prediction: <em style="color:green">Embarrassment</em> </h4>

      </div>
  
      <div class="col-sm-4">
        <h4> Minus Environment: <h4 style = "color:#800084"> Jane is a female adult. Jane is or has taking off eyeglasses. Mia is Jane's daughter and Jane is putting her hand on Mia's shoulder while Mia has her back turned to Jane. Jane is likely feeling....?</h4>
          <h4>GPT Prediction: <em style="color:green">Love</em> </h4>
        </h4>
      </div>
    
    </div>
    <!-- <h4>Notable Observations: <h5>Fear is an emotion that may benefit from environmental context. The F1 score for this image dropped from 0.52 and 0.48 with
      environments to 0.34 without environments. An example is
      shown in this Figure, when "right in front of an alien hand in
      the dark" was removed from the full caption, the predicted emotion changed
      from Fear to Disapproval. </h5></h4> -->

   
    
    


    

  <!-- <div  style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;"> -->
    <!-- <h4 style="padding-bottom:2rem"><u>Experimental Results</u></h4> -->
    <!-- <div class="row">
      <div class="col-sm-6">
        <img src="resrc/img1.png" alt="dti.png"  style="width: 100%; max-width: 1000px">
      </div>
      
      <div class="col-sm-6">
        <img src="resrc/img3.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1000px">
      </div>
      <div class="col">
        <img src="resrc/img2.png" alt="deep_tsf.png"  style="width: 110%; max-width: 650px; text-align: center;">
      </div>
    </div> -->
    



  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-sm-6">
      <h4>Paper</h4>
      <a href="" style="color:inherit">
        <img src="resrc/paper.png" alt="paper.png" class="text-center" style="max-width:70%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-sm-6 ">
      <h4>Annotation Website</h4>
      <a href="https://dev9032.d1ab6be83l9u2m.amplifyapp.com/" style="color:inherit;">
        <img src="resrc/website_teaser.png" alt="github_repo.png" class="text-center"
             style="max-width:70%;border:0.15em solid;
             border-radius:0.5em;padding-top:102px ;padding-bottom: 80px; "></a>
    </div>
    <!-- <div class="col-xs-4 col-lg-4">
      <h4>Slides</h4>
      <a href="dtic_long.pptx" style="color:inherit;">
        <img src="resrc/slides.png" alt="slides.png" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div> -->
    <div class="col-xs-0 col-lg-0"></div>
  </div>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
@inproceedings{yang2023contextual,
  title={Contextual Emotion Estimation from Image Captions},
  author={Yang, Vera AND Srivastava, Archita AND Etesam, Yasaman AND Zhang, Shay AND Lim, Angelica},   
  booktitle={International Conference on Affective Computing and Intelligent Interaction (ACII)},
  year={2023},
}
</pre>
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2104.14575">Monnier et al. - Unsupervised Layered Image Decomposition into Object
        Prototypes (arXiv 2021)</a>
    </li>
  </ul>

  <h4>Previous works on deep transformations</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1908.04725">Deprelle et al. - Learning elementary structures for 3D shape
        generation and matching (NeurIPS 2019)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1806.05228">Groueix et al. - 3D-CODED: 3D Correspondences by Deep Deformation (ECCV
        2018)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1802.05384">Groueix et al. - AtlasNet: A Papier-Mache Approach to Learning 3D
        Surface Generation (CVPR 2018)</a>
    </li>
  </ul> -->


  <!-- <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported in part by <a href="https://enherit.enpc.fr/">ANR project EnHerit</a> ANR-17-CE23-0008,
    project Rapid Tabasco, gifts  from  Adobe and HPC resources from GENCI-IDRIS (Grant 2020-AD011011697). We thank 
    Bryan Russell, Vladimir Kim, Matthew Fisher, Fran&#231;ois Darmon, Simon Roburin, David Picard, Michael 
    Ramamonjisoa, Vincent Lepetit, Elliot Vincent, Jean Ponce, William Peebles and Alexei Efros for inspiring 
    discussions and valuable feedback.
  </p> -->
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
