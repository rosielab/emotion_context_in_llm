 <!DOCTYPE html>
<html lang="en">
<head>
  <title>Contextual Emotion Estimation</title>
  <meta name="description" content="Project page for Contextual Emotion Estimation from Image Captions.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <!-- <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/> -->

  <!--Twitter preview-->
  <!-- <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png"> -->

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Contextual Emotion Estimation from Image Captions</h1>
    <h4>ACII 2023</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>

        <!-- How to write authors and how to do we need links?-->
        <nobr>Vera Yang</nobr><sup>*</sup> &emsp;
        <nobr>Archita Srivastava</nobr><sup>*</sup> &emsp;

        
        <a href=""><nobr>Yasaman Etesam</nobr><sup></sup> </a>&emsp;
        <a href=""><nobr>Shay Zhang</nobr><sup></sup></a>&emsp;
        <a href=""><nobr>Angelica Lim</nobr><sup></sup> </a>&emsp;
        <!-- <nobr>Hongwei Zhao</nobr><sup>1</sup> &emsp;
        <nobr>Hongtao Lu</nobr><sup>2</sup> &emsp; -->
        <!-- <a href="https://xishen0220.github.io/"><nobr>Xi Shen</nobr><sup>3</sup></a> -->
      </h4>
      <sup>*</sup> Equal Contribution &emsp;
      <p><em><center>School of Computing Science, Simon Fraser University, Burnaby, BC, Canada  &emsp;</center></em></p>
      <!-- <sup>2</sup> Shanghai Jiao Tong University &emsp;
      <sup>3</sup> Tencent AI Lab, Shenzhen -->
      <!-- Rosie Labs, <nobr>Simon Fraser University</nobr>, <nobr>Burnaby</nobr>, British Columbia,
      <nobr>Canada</nobr> -->
    </div>
    <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2006.11132.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/monniert/dti-clustering" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.png" alt="teaser.png" class="text-center" style="width: 110%; max-width: 2000px">
  <p><h5><em>Manual Annotation for the given image produces the following caption:</em> Sean is a male adult. Sean is a(n) passenger. Sean is or has raising eyebrows,
    side-eyeing. Mia is a child and she is sitting behind Sean and kicking Sean’s chair. Sean’s physical environment is on an airplane.</h5></p>
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://arxiv.org/abs/2006.11132">Paper</a>
    <a class="label label-info" href="https://github.com/monniert/dti-clustering">Code</a>
    <a class="label label-info" href="https://www.youtube.com/embed/j20MBc1hWGQ">Video</a>
    <a class="label label-info" href="resrc/dtic_long.pptx">Slides</a>
    <a class="label label-info" href="resrc/ref.bib">BibTeX</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
          Traditional methods attempt to estimate people’s
      emotions in images by solely focusing on the face and body
      pose. However, previous research has proven that relying only
      on this information is inadequate. On the other hand, Large
      Language Models (LLMs) possess information about various
      aspects of humans. But how well do LLMs perceive human
      emotions? And which parts of the information enable them
      to accurately determine emotions? We tackle the challenging
      contextual image emotion estimation task by using text captions
      as a rich and interpretable representation of the image. One
      major challenge is to construct a caption that describes a person
      within a scene with information relevant for emotion perception,
      rather than detecting objects. Towards this goal, we propose a
      set of natural language descriptors for faces, bodies, interactions,
      and environments. Furthermore, we annotate a subset of images
      from the EMOTIC dataset with these descriptors and test the
      capability of a large language model to infer an emotion from the
      resulting image caption. We find that GPT-3, specifically the text-
      davinci-003 model, provides surprisingly reasonable predictions
      consistent with human annotations
  </p>

  <!-- <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-6 text-center">
      <h4><u>Short presentation</u> (3min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/j20MBc1hWGQ" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>

    <div class="col-xs-6 text-center">
      <h4><u>Long presentation</u> (11min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/xhLUOh5PKBA" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

  
  <h3>Approach</h3>
  <hr/>
    
  <div class="row" style="text-align: center">
    <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <h4><u>Image Annotations</u></h4>
        <img src="resrc/annotate.png" alt="pr
        ototypes.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
    </div>

    <div class="row" style="text-align: center">
      <div class="col-xs-6">
        <h4><u>Generated Captions</u></h4>
      </div>
      <div class="col-xs-6">
        <h4><u>Samples</u> 
      </div>
    </div>
    <div class="row" style="text-align: center">
      <div class="col-xs-6">
        <img src="resrc/captions.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 900px">
      </div>
      <div class="col-xs-6">
        <img src="resrc/samples.png" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 900px">
      </div>
    </div>

    <div style="padding-top:3rem">
      <p> Once the annotations were complete, GPT-3 was used
        to predict emotion labels with the help of a prompt. The
        prompt was structured to elicit single emotion prediction when
        presented with an image annotation.
        The prompt was as follows: <em>”Sean is a male adult. Sean is a(n) passenger. Sean
        is or has raising eyebrows, side-eyeing. Mia is a child and
        she is sitting behind Sean and kicking Sean’s chair. Sean’s
        physical environment is on an airplane. Sean is likely feeling
        a high level of {placeholder}? Choose one emotion from
        the list: Anger, Annoyance, Aversion, Confusion, Disapproval,
        Disconnection, Disquietment, Embarrassment, Fatigue, Fear,
        Pain/Suffering (emotional), Pain/Suffering (physical), and Sadness.”</em>
        To evaluate the performance of the models, we compared
        the LLM’s predictions to the ground truth of the images established by the annotators</p>
    </div>
  </div>
  
     

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>GPT-3 Results</u></h4>
      <img src="resrc/gptresults.png" alt="pr
      ototypes.jpg" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div  style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4 style="padding-bottom:2rem"><u>Experimental Results</u></h4>
    <div class="row">
      <div class="col-sm-4">
        <img src="resrc/exp1.png" alt="dti.png"  style="width: 110%; max-width: 1100px">
      </div>
      <div class="col-sm-4">
        <img src="resrc/exp2.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1100px">
      </div>
      <div class="col-sm-4">
        <img src="resrc/exp3.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1100px">
      </div>
    </div>
  </div>
  
  <h3>Some Visual Results</h3>
  <hr/>
  <div  style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <!-- <h4 style="padding-bottom:2rem"><u>Experimental Results</u></h4> -->
    <div class="row">
      <div class="col-sm-6">
        <img src="resrc/img1.png" alt="dti.png"  style="width: 100%; max-width: 1000px">
      </div>
      
      <div class="col-sm-6">
        <img src="resrc/img3.png" alt="deep_tsf.png"  style="width: 110%; max-width: 1000px">
      </div>
      <div class="col">
        <img src="resrc/img2.png" alt="deep_tsf.png"  style="width: 110%; max-width: 650px; text-align: center;">
      </div>
    </div>
    
  </div>



  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-sm-6">
      <h4>Paper</h4>
      <a href="https://arxiv.org/abs/2006.11132" style="color:inherit">
        <img src="resrc/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:70%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-sm-6 ">
      <h4>Annotation Website</h4>
      <a href="https://github.com/monniert/dti-clustering" style="color:inherit;">
        <img src="resrc/github_repo.png" alt="github_repo.png" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <!-- <div class="col-xs-4 col-lg-4">
      <h4>Slides</h4>
      <a href="dtic_long.pptx" style="color:inherit;">
        <img src="resrc/slides.png" alt="slides.png" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div> -->
    <div class="col-xs-0 col-lg-0"></div>
  </div>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
@inproceedings{vera2023acii,
  title={{Contextual Emotion Estimation from
    Image Captions}},
  author={Vera, Archita, Yasaman, Shay and Angelics},
  booktitle={International Conference on Affective Computing and Intelligent Interaction (ACII)},
  year={2023},
}</pre>
      </div>
    </div>

  <!-- <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2104.14575">Monnier et al. - Unsupervised Layered Image Decomposition into Object
        Prototypes (arXiv 2021)</a>
    </li>
  </ul>

  <h4>Previous works on deep transformations</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1908.04725">Deprelle et al. - Learning elementary structures for 3D shape
        generation and matching (NeurIPS 2019)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1806.05228">Groueix et al. - 3D-CODED: 3D Correspondences by Deep Deformation (ECCV
        2018)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1802.05384">Groueix et al. - AtlasNet: A Papier-Mache Approach to Learning 3D
        Surface Generation (CVPR 2018)</a>
    </li>
  </ul> -->


  <!-- <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported in part by <a href="https://enherit.enpc.fr/">ANR project EnHerit</a> ANR-17-CE23-0008,
    project Rapid Tabasco, gifts  from  Adobe and HPC resources from GENCI-IDRIS (Grant 2020-AD011011697). We thank 
    Bryan Russell, Vladimir Kim, Matthew Fisher, Fran&#231;ois Darmon, Simon Roburin, David Picard, Michael 
    Ramamonjisoa, Vincent Lepetit, Elliot Vincent, Jean Ponce, William Peebles and Alexei Efros for inspiring 
    discussions and valuable feedback.
  </p> -->
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
